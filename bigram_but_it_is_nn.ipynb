{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "names_file = open(\"./data/names.txt\", \"r\")\n",
    "G = torch.Generator().manual_seed(42)\n",
    "names_all = names_file.read().split(\"\\n\")\n",
    "names_train, names_dev, names_test = torch.utils.data.random_split(names_all, [0.8, 0.1, 0.1], generator=G)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture of network will be 27 neurons with 27 weights each thus they will simulate table from bigram (27x27)\n",
    "the input will one_hot vector \n",
    "so one hot vector will be applied to every neuron but only one weight from neuron will be used rest will be zero\n",
    "then we have to move this numbers to something that looks life probability <- softmax\n",
    "then maximum likelihood as cost function\n",
    "and finally gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_CHAR = '<S>'\n",
    "LAST_CHAR = '<E>'\n",
    "\n",
    "all_letters = set()\n",
    "for name in names_all:\n",
    "    normalized_name = list(name)\n",
    "    for char in normalized_name:\n",
    "        all_letters.add(char)\n",
    "\n",
    "letter_to_int32 = {letter: index for index, letter in enumerate(all_letters)}\n",
    "letter_to_int32[FIRST_CHAR] = 26\n",
    "letter_to_int32[LAST_CHAR] = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for name in names_train:\n",
    "    normalized_name = [FIRST_CHAR] + list(name) + [LAST_CHAR]\n",
    "    for ch1, ch2 in zip(normalized_name, normalized_name[1:]):\n",
    "        xs.append(letter_to_int32[ch1])\n",
    "        ys.append(letter_to_int32[ch2])\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "xs_enc = torch.nn.functional.one_hot(xs, num_classes=28).float()\n",
    "ys_enc = torch.nn.functional.one_hot(ys, num_classes=28).float()\n",
    "# weights initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(3.7961, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6389, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5103, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4039, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3147, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2388, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1734, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1164, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0663, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0220, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9825, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9474, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9159, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8878, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8625, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8399, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8194, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8009, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7841, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7688, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7548, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7419, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7300, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7190, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7088, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6993, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6905, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6822, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6744, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6671, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6602, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6537, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6476, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6419, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6364, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6313, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6264, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6218, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6174, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6132, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6093, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6055, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6019, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5984, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5951, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5920, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5890, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5861, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5833, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5807, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5781, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5756, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5733, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5710, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5688, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5666, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5646, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5626, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5607, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5588, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5570, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5553, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5536, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5519, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5503, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5488, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5473, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5458, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5444, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5430, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5416, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5403, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5390, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5378, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5366, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5354, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5343, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5331, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5320, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5310, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5299, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5289, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5279, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5269, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5260, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5250, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5241, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5233, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5224, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5215, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5207, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5199, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5191, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5183, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5176, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5168, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5161, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5154, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5147, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5140, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((28,28), requires_grad=True, generator=G)\n",
    "\n",
    "for _ in range(100):\n",
    "    # in one step we evaluate whole model in parallel\n",
    "    evaluated  = W[xs]\n",
    "\n",
    "    # forward \n",
    "\n",
    "    # maximum likehood\n",
    "    # evaluated_exp = evaluated.exp()\n",
    "    # prob = evaluated_exp / evaluated_exp.sum(1, keepdims=True)\n",
    "    #loss = prob[xs, ys].log().mean() * -1 \n",
    "  \n",
    "    # cross entropy\n",
    "    loss = torch.nn.functional.cross_entropy(evaluated, ys)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -20 * W.grad\n",
    "    W.grad = None\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(3.7773, grad_fn=<MulBackward0>)\n",
      "loss=tensor(3.5775, grad_fn=<MulBackward0>)\n",
      "loss=tensor(3.4165, grad_fn=<MulBackward0>)\n",
      "loss=tensor(3.2855, grad_fn=<MulBackward0>)\n",
      "loss=tensor(3.1811, grad_fn=<MulBackward0>)\n",
      "loss=tensor(3.0996, grad_fn=<MulBackward0>)\n",
      "loss=tensor(3.0358, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.9848, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.9430, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.9081, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.8788, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.8540, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.8329, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.8149, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7993, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7856, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7735, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7627, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7529, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7440, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7359, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7285, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7216, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7152, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7093, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7038, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6986, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6938, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6894, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6852, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6812, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6775, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6741, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6708, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6678, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6649, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6622, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6597, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6573, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6550, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6529, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6509, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6489, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6471, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6454, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6437, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6422, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6407, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6392, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6379, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6366, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6353, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6341, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6330, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6319, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6308, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6298, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6288, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6279, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6270, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6261, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6253, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6244, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6237, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6229, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6222, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6215, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6208, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6201, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6195, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6189, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6183, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6177, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6171, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6166, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6161, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6156, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6151, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6146, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6142, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6137, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6133, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6128, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6124, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6120, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6116, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6113, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6109, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6105, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6102, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6099, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6095, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6092, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6089, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6086, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6083, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6080, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6077, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6074, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6072, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# maximum likehood\n",
    "W = torch.randn((28,28), requires_grad=True, generator=G)\n",
    "\n",
    "for _ in range(100):\n",
    "    # in one step we evaluate whole model in parallel\n",
    "    evaluated  = W[xs]\n",
    "\n",
    "    # forward \n",
    "    evaluated_exp = evaluated.exp()\n",
    "    prob = evaluated_exp / evaluated_exp.sum(1, keepdims=True)\n",
    "    loss = prob[xs, ys].log().mean() * -1 \n",
    "  \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -20 * W.grad\n",
    "    W.grad = None\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(10.6648, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.2466, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.6062, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.8986, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.7238, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8207, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0288, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4917, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2539, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0679, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9507, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8928, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8207, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7956, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7463, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7269, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6952, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6858, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6593, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6549, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6346, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6331, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6158, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6160, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6012, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6023, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5893, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5910, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5794, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5814, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5709, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5731, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5634, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5656, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5567, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5589, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5506, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5527, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5450, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5470, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5398, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5417, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5350, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5368, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5305, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5322, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5263, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5279, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5224, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5239, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# with 2 layers\n",
    "W1 = torch.randn((28,28), requires_grad=True, generator=G)\n",
    "W2 = torch.randn((28,28), requires_grad=True, generator=G)\n",
    "\n",
    "for _ in range(50):\n",
    "    # in one step we evaluate whole model in parallel\n",
    "    evaluated  = xs_enc @ W1 @  W2\n",
    "\n",
    "    # forward \n",
    "    loss = torch.nn.functional.cross_entropy(evaluated, ys) \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    W1.data += -5 * W1.grad\n",
    "    W1.grad = None\n",
    "\n",
    "    W2.data += -5 * W2.grad\n",
    "    W2.grad = None\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "\n",
    "model = xs_enc  @ W1 @ W2\n",
    "model_exp = evaluated.exp()\n",
    "P = model_exp / model_exp.sum(1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mieewxzil\n",
      "<E>llmhytngeaa\n",
      "vwj\n",
      "<E>moxhdzea\n",
      "h\n",
      "lye\n",
      "bshycd\n",
      "<E>\n",
      "eii\n",
      "wtaykoknoao<S>r\n",
      "tensor([0.0063, 0.0834, 0.0030, 0.0198, 0.0980, 0.0042, 0.0033, 0.0354, 0.0206,\n",
      "        0.0183, 0.0024, 0.0180, 0.0109, 0.0056, 0.0320, 0.0251, 0.0035, 0.1051,\n",
      "        0.0684, 0.0644, 0.0098, 0.0228, 0.0135, 0.0383, 0.0033, 0.0588, 0.0016,\n",
      "        0.2243], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "int32_to_letter = {value: key for key,value in letter_to_int32.items()}\n",
    "def predict_next_letter_index(letter_index):\n",
    "    prob_dist = P[letter_index]\n",
    "    return torch.multinomial(prob_dist, num_samples=1).item()\n",
    "\n",
    "\n",
    "\n",
    "def create_word():\n",
    "    idx = 26\n",
    "    word = []\n",
    "    while True:\n",
    "        idx = predict_next_letter_index(idx)\n",
    "        word.append(int32_to_letter[idx])\n",
    "        if idx == 27 and len(word) > 1:\n",
    "            return ''.join(word[:-1])\n",
    "        \n",
    "for _ in range(10):\n",
    "    print(create_word())\n",
    "\n",
    "print(P[26])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
