{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "names_file = open(\"./data/names.txt\", \"r\")\n",
    "G = torch.Generator().manual_seed(42)\n",
    "names_all = names_file.read().split(\"\\n\")\n",
    "names_train, names_dev, names_test = torch.utils.data.random_split(names_all, [0.8, 0.1, 0.1], generator=G)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture of network will be 27 neurons with 27 weights each thus they will simulate table from bigram (27x27)\n",
    "the input will one_hot vector \n",
    "so one hot vector will be applied to every neuron but only one weight from neuron will be used rest will be zero\n",
    "then we have to move this numbers to something that looks life probability <- softmax\n",
    "then maximum likelihood as cost function\n",
    "and finally gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_CHAR = '<S>'\n",
    "LAST_CHAR = '<E>'\n",
    "\n",
    "all_letters = set()\n",
    "for name in names_train:\n",
    "    normalized_name = [FIRST_CHAR] + list(name) + [LAST_CHAR]\n",
    "    for char in normalized_name:\n",
    "        all_letters.add(char)\n",
    "\n",
    "letter_to_int32 = {letter: index for index, letter in enumerate(all_letters)}\n",
    "letter_to_int32[FIRST_CHAR] = len(letter_to_int32)\n",
    "letter_to_int32[LAST_CHAR] = len(letter_to_int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for name in names_train:\n",
    "    normalized_name = [FIRST_CHAR] + list(name) + [LAST_CHAR]\n",
    "    for ch1, ch2 in zip(normalized_name, normalized_name[1:]):\n",
    "        xs.append(letter_to_int32[ch1])\n",
    "        ys.append(letter_to_int32[ch2])\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "xs_enc = torch.nn.functional.one_hot(xs, num_classes=29).float()\n",
    "ys_enc = torch.nn.functional.one_hot(ys, num_classes=29).float()\n",
    "# weights initialization\n",
    "W = torch.randn((29,29), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(3.7950, grad_fn=<MulBackward0>)\n",
      "loss=tensor(3.3277, grad_fn=<MulBackward0>)\n",
      "loss=tensor(3.0827, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.9445, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.8612, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.8081, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7704, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7421, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7201, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.7026, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6883, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6765, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6668, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6586, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6518, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6460, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6410, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6368, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6330, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6298, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6269, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6243, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6219, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6198, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6179, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6161, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6145, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6130, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6116, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6103, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6091, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6079, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6069, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6059, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6049, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6040, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6032, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6024, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6016, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6009, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.6002, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5996, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5990, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5984, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5978, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5972, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5967, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5962, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5957, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5953, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5949, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5944, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5940, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5936, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5933, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5929, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5925, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5922, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5919, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5916, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5913, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5910, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5907, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5904, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5902, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5899, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5897, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5894, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5892, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5890, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5888, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5886, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5884, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5882, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5880, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5878, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5876, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5874, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5873, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5871, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5869, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5868, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5866, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5865, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5863, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5862, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5860, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5859, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5858, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5856, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5855, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5854, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5853, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5852, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5851, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5849, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5848, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5847, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5846, grad_fn=<MulBackward0>)\n",
      "loss=tensor(2.5845, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    # in one step we evaluate whole model in parallel\n",
    "    evaluated = xs_enc @ W\n",
    "\n",
    "    # forward \n",
    "    evaluated_exp = evaluated.exp()\n",
    "    prob = evaluated_exp / evaluated_exp.sum(1, keepdims=True)\n",
    "    loss = prob[xs, ys].log().mean() * -1 \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -50 * W.grad\n",
    "    W.grad = None\n",
    "\n",
    "\n",
    "    print(f'{loss=}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
