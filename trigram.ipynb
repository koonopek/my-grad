{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "names_file = open(\"./data/names.txt\", \"r\")\n",
    "G = torch.Generator().manual_seed(42)\n",
    "names_all = names_file.read().split(\"\\n\")\n",
    "\n",
    "names_train, names_dev, names_test = torch.utils.data.random_split(names_all, [0.8, 0.1, 0.1], generator=G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_CHAR = '<S>'\n",
    "LAST_CHAR = '<E>'\n",
    "\n",
    "all_letters = set()\n",
    "for name in names_train:\n",
    "    for char in name:\n",
    "        all_letters.add(char)\n",
    "\n",
    "letter_to_int32 = {letter: index for index, letter in enumerate(all_letters)}\n",
    "letter_to_int32[FIRST_CHAR] = len(letter_to_int32)\n",
    "letter_to_int32[LAST_CHAR] = len(letter_to_int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "int32_to_letter = {value: key for key,value in letter_to_int32.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(names_dev)=tensor(2.1169)\n",
      "loss(names_train)=tensor(2.0666)\n",
      "loss(names_test)=tensor(2.1176)\n"
     ]
    }
   ],
   "source": [
    "letters_count = len(letter_to_int32)\n",
    "N = torch.zeros((letters_count, letters_count, letters_count))\n",
    "\n",
    "for name in names_train:\n",
    "    normalized_name = [FIRST_CHAR, FIRST_CHAR] + list(name) + [LAST_CHAR]\n",
    "    for ch1, ch2, ch3 in zip(normalized_name, normalized_name[1:], normalized_name[2:]):\n",
    "        int_ch1 = letter_to_int32[ch1]\n",
    "        int_ch2 = letter_to_int32[ch2]\n",
    "        int_ch3 = letter_to_int32[ch3]\n",
    "        N[int_ch1,int_ch2, int_ch3] += 1\n",
    "\n",
    "# maximum likelihood mul(P[x][i]) => log sum(log(P[x][i])) => negative -sum(log(P[x][i]))\n",
    "# regularize N to avoid situation where we have 0 prob that some bigram occurs after another one\n",
    "N = N + 0.2\n",
    "# normalize to probability row wise\n",
    "P = N / N.sum(dim=2, keepdim=True)\n",
    "\n",
    "def loss(set):\n",
    "    loss = 0\n",
    "    n = 0\n",
    "    for name in set:\n",
    "        normalized_name = [FIRST_CHAR] + list(name) + [LAST_CHAR]\n",
    "        for ch1, ch2, ch3 in zip(normalized_name, normalized_name[1:], normalized_name[2:]):\n",
    "            int_ch1 = letter_to_int32[ch1]\n",
    "            int_ch2 = letter_to_int32[ch2]\n",
    "            int_ch3 = letter_to_int32[ch3]\n",
    "\n",
    "            loss += torch.log(P[int_ch1, int_ch2, int_ch3])\n",
    "            n += 1\n",
    "\n",
    "    return -loss/n\n",
    "\n",
    "print(f'{loss(names_dev)=}')\n",
    "print(f'{loss(names_train)=}')\n",
    "\n",
    "print(f'{loss(names_test)=}')\n",
    "\n",
    "# 0.1\n",
    "#loss(names_dev)=tensor(2.1169)\n",
    "#loss(names_train)=tensor(2.0617)\n",
    "\n",
    "# 5\n",
    "#loss(names_dev)=tensor(2.2166)\n",
    "#loss(names_train)=tensor(2.1894)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
